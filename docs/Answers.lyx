#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsart
\begin_preamble
\usepackage{algorithm,algpseudocode}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 1cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 2cm
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Box Doublebox
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.35pt"
separation "9pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset VSpace 1mm
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Title
ADVANCED PRACTICAL MACHINE LEARNING
\end_layout

\begin_layout Title
Reinforcement Learning
\begin_inset Newline newline
\end_inset

Connect Four
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace 3mm
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\align center

\size large
Hanan Benzion 
\begin_inset Formula $\quad$
\end_inset

026606608 
\end_layout

\begin_layout Plain Layout
\align center

\size large
Idan Azuri
\begin_inset Formula $\quad$
\end_inset

302867833
\end_layout

\begin_layout Plain Layout
\align center

\size footnotesize
\begin_inset ERT
status open

\begin_layout Plain Layout

{
\backslash
footnotesize
\backslash
today}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace 6mm
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section

\size larger
Practical Results
\end_layout

\begin_layout Subsubsection*

\series bold
Board representation
\end_layout

\begin_layout Standard
We chose to represent the board as a vector in dimension (batch_size, 42).
 The optimal board representation should be smooth so it will be possible
 to use SGD to optimization for our objective function.
 Another parameter that we considered is the minimal divergence rate is
 also possible for all games, but unlike with smoothness is actually practical
 for a reasonable number of games.
 Essentially optimality can be achieved by ensuring that any state 
\begin_inset Formula $S'$
\end_inset

 derivable from state 
\begin_inset Formula $S$
\end_inset

 has the property that 
\begin_inset Formula $|S-S'|=1$
\end_inset

.
 In that manner also (6,7) shape would be a good choice, because it has
 this property.
 So the consideration of choosing this shape was to simplify our deep neural
 network and and to make an easier representation in our code 
\end_layout

\begin_layout Subsubsection*

\series bold
The Algorithm
\end_layout

\begin_layout Standard
\paragraph_spacing double
The Q-Network is a method based on Q-Learning which is trying to predict
 the maximum expected cumulative reward for a given a pair(state 
\begin_inset Formula $s$
\end_inset

, action 
\begin_inset Formula $a$
\end_inset

).
 Formally' 
\begin_inset Formula 
\[
Q^{*}(s',\,a)=\underset{\pi}{Max}E\left[\sum_{t\geq0}\gamma^{t}r_{t}|s_{0}=s,\,a_{0}=a,\,\pi\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing double
\begin_inset Formula $\pi-the\,policy\quad s^{'}-next\,state\quad a-action\quad\gamma-uncertainty\,value$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $Q*$
\end_inset

 satisfies the following Bellman equation uncertainty 
\begin_inset Formula 
\[
Q^{*}(s',,\,a)=\mathbb{E}_{s'\backsim\varepsilon}\left[r\,+\,\gamma maxQ^{*}(s',a')|s,a\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The intuition is, if the optimal state-action values for the next step 
\begin_inset Formula $Q^{*}(s',\,a)$
\end_inset

 are known, then the optimal strategy is to taken the action that maximizes
 the expected value of 
\begin_inset Formula $r+\gamma Q^{*}(s',,a')$
\end_inset

.
 Now we can not learn the entire space of the Q function, 
\begin_inset Formula $S\times A$
\end_inset

 states and actions because it is too large 
\begin_inset Formula $O(2^{42})$
\end_inset

.
 So the Deep Q learning is an approximation for this Q function where the
 Forward pass is defined as follows, 
\begin_inset Formula 
\begin{equation}
L_{i}(\theta_{i})=\mathbb{E}_{s,a\backsim p(\dot{})}\left[\left(y_{i}-Q\left(s,a;\theta_{i}\right)\right)^{2}\right]\label{eq:1}
\end{equation}

\end_inset

 where 
\begin_inset Formula 
\begin{equation}
y_{i}=\mathbb{E}_{s^{i}\backsim\varepsilon}\left[r+\gamma\underset{a^{'}}{Max}Q\left(s^{'},a^{'};\theta_{i-1}|s,a\right)\right]\label{eq:2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
and the Backward pass is
\begin_inset Formula 
\[
\nabla_{\theta_{i}}L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s_{i},a\backsim p(\dot{});s^{'}\backsim\varepsilon}\left[r\,+\,\gamma\,Max_{a^{'}}Q^{*}(s',a';\theta_{i-1})-Q\left(s,a;\theta_{i}\right)\nabla_{\theta_{i}}Q\left(s,a;\theta_{i}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
We chose the Deep Q-Network method, our mainly consideration to chose this
 method was the efficiency of this approach, because it is needed only one
 feedforward pass to compute the Q-values for all actions from a given current
 state.
 The efficiency is very important in this task due to the 
\begin_inset Quotes eld
\end_inset

online learning
\begin_inset Quotes erd
\end_inset

 constraint that we had to deal with.
 Moreover it is very intuitive so you can see the learning at a quit early
 stage of the training and it is an easier method to debug comparing to
 the other methods (i.e.
 policy gradients would be much more difficult to understand wether the
 model works well).
 
\end_layout

\begin_layout Subsubsection*

\series bold
Our model
\end_layout

\begin_layout Subsubsection*
General description
\end_layout

\begin_layout Standard
Our model is an online Q-Network where it plays many games against itself
 (or another opponent), it saves the values of a quartet of 
\begin_inset Formula $s^{'},s,a,r$
\end_inset

 (next_state, state, action, reward) and it learn every end game.
 The learning stage can be treated as a regression problem where we are
 trying to find the optimal value of the 
\begin_inset Formula $max$
\end_inset


\begin_inset Formula $Q^{*}(s',a')$
\end_inset

 as described above 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:2"

\end_inset

.
 We chose to work with a Deep Neural Network (DNN) from the reason we can
 frame the problem of estimating 
\begin_inset Formula $Q(s,a)$
\end_inset

 as a simple regression problem.
 Given an input vector consisting of 
\begin_inset Formula $s$
\end_inset

 and 
\begin_inset Formula $a$
\end_inset

 the neural net is supposed to predict the a value of 
\begin_inset Formula $Q(s,a)$
\end_inset

 equal to 
\begin_inset Formula $target:r+\mathbb{\gamma}*maxQ(s’,a’)$
\end_inset

.
 If we are good at predicting
\begin_inset Formula $Q(s,a)$
\end_inset

 for different states 
\begin_inset Formula $S$
\end_inset

 and actions 
\begin_inset Formula $A$
\end_inset

, we have a good approximation of 
\begin_inset Formula $Q$
\end_inset

.
 Note that
\begin_inset Formula $Q(s’,a’)$
\end_inset

is also a prediction of the neural network we are training.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1] 
\end_layout

\begin_layout Plain Layout


\backslash
State{batch $
\backslash
gets$ sample a random batch}
\backslash
Comment{Sample a random batch from the ExperienceReplay DB}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
While{left samples in batch}
\end_layout

\begin_layout Plain Layout


\backslash
State{For each possible action a'(1,2,3,4,5,6,7),predict expected future
 reward $Q_{t}$ using DNN}
\end_layout

\begin_layout Plain Layout


\backslash
State{Choose the highest value of the actions predictions $max_{a} Q(s',a')$}
 
\backslash
Comment in our case we have 7 actions
\end_layout

\begin_layout Plain Layout


\backslash
State{Calculate $r+ 
\backslash
gamma
\backslash
times max_Q(s',a')$.
 This is the target value for the DNN.}
\end_layout

\begin_layout Plain Layout


\backslash
State{Train the neural net using the loss function $1/2(predicted_Q(s,a)
 - target)^2$}
\end_layout

\begin_layout Plain Layout


\backslash
EndWhile 
\end_layout

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Pseudo of our the learning stage in our model
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Model architecture
\end_layout

\begin_layout Standard
We designed a DNN model like regression problem where the loss is defined
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:1"

\end_inset

.
 We chose a relatively small network in order to keep the number of parameters
 small yet, not to small because we do want to generalize the Q function
 well.
 The architecture is, 
\begin_inset Formula 
\[
LeakyRelu(Dense(Input,128))\implies LeakyRelu(Dense(128,64))\implies\text{LeakyRelu(Dense(64,64))\implies TanH(Dense(64,32))\implies Dense(32,actions))}
\]

\end_inset


\end_layout

\begin_layout Standard
where input - 
\begin_inset Formula $(batch\,size,42)\;;\,\,actions-(7,)$
\end_inset


\end_layout

\begin_layout Subsubsection
The importance of the move
\end_layout

\begin_layout Standard
We noticed that most important sample is the sample from the last move in
 the game, so in order to increase our dataset quality we created a 
\begin_inset Quotes eld
\end_inset

Balanced experience replay
\begin_inset Quotes erd
\end_inset

 .
 The balanced database has two different memories, one for regular moves,
 and one for the last movs in the game.
 We used that property to create balanced batch which has the same amount
 of regular moves as the last moves.
 For example for batch_size=16 we split it to 8-regular moves, 8-last moves.
 This trick showed an improvement on relatively small number of training
 iterations.
\end_layout

\begin_layout Subsubsection*
The invert trick
\end_layout

\begin_layout Standard
We thought about how to teach the model more last moves without playing
 the entire game, or maybe how can we teach the model to block the opponent
 when he is closed to win.
 We came up with an idea the invert the board hence switch between player
 1 and player 2 pieces TODO****
\end_layout

\begin_layout Subsubsection*
Experience replay
\end_layout

\begin_layout Standard
We sampled a quartet of 
\begin_inset Formula $s^{'},s,a,r$
\end_inset

 (next_state, state, action, reward) from all the played moves in the previous
 games and store them, we also stored the last move of the game (before
 someone wins or losses), Then in the training we sampled a random shuffled
 batches from this dataset, (as a side note, it is very important to sample
 each batch where its samples are not consecutive in order to avoid wrong
 policy learning i.e.
 bad feedback loops).
\end_layout

\begin_layout Subsubsection*

\series bold
Exploration-Exploitation Trade-off
\end_layout

\begin_layout Standard
One problem found in many areas of AI is the exploration problem.
 Should the agent always preform the best move available or should it take
 risks and “explore” by taking potentially suboptimal moves in the hope
 that they will actually lead to a better outcome (which can then be learnt
 from).
 A game that forces explorations is one where the inherent dynamics of the
 game force the player to explore, thus removing the choice from the agent.
\end_layout

\begin_layout Standard
In our model we used exploration rate of 10% which decays 
\begin_inset Formula $\times2$
\end_inset

 every 5000 rounds all the way to 0.1% and it stays there for the rest of
 the game.
 We didn't have enough time to benchmark the different values of exploration-exp
loitation values but as a rule of thumb you do not want to eliminate the
 exploration in any stage of the game, yet you do not want to let the exploratio
n a big portion of the moves, according to that we determined this ratio.
\end_layout

\begin_layout Subsubsection*

\series bold
A short description of the tests and results you got when you trained your
 policy 
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Subsubsection*

\series bold
A detailing of other possible solutions that you tried and decided not to
 hand in and why (if were are any).
 
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Subsubsection*

\series bold
Any other things you would like to note about your implementation
\end_layout

\begin_layout Standard
TODO
\end_layout

\end_body
\end_document
